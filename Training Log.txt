Training Log
agent_trained01.pt - 300 episodes, same starting position on the large track. Model follows track well but drives slow
agent_trained02.pt - 500 Episodes, 0-10 acceleration command bounds. Random initial position along the path 
agent_trained03.pt - 400 Episodes, 0-1.5 acceleration command bounds. Random initial position along the path
agent_trained04.pt - 1000 Episodes, 0-1 acceleration command bounds. Random initial position. 800 Episode length. Standard Reward (Merge CDD for agent file)
agent_trained05_CDD.pt - 1000 Episodes, 0-1 acceleration command bounds. Random initial position. 800 Episode length. Standard Reward. New Occupancy grid representation (channels), No action noise during training. Doesn't drive the track well. 
agent_trained06.pt - 1000 episodes 0-1 acceleration commands. Random initial position. 800 Episode length. Standard Reward. Updated reward weights. Returned to 0.001 learning rates. Added function to ensure vehicle doesn't spawn on an obstacle.
agent_trained07.pt - -1-1 acceleration commands. Random Initial Position, 800 episode length, standard Reward. Corrected orientation error and path representation on occupancy grid. Good performance. A bit jumpy, but achieves the objectives. Appears as if it has learned to keep a buffer around the obstacles.
agent_trained08.pt - Trained on agent07 + 300 episodes. Smooth reward, higher speed (2.0). Actor loss seems to go up but the agent does pretty well in inference. Need to train from 0 with reward tracking for better insights.